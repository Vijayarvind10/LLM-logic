# LLM-logic
### **Setting up Local LLM Tools: ChatGPT API and SWI-Prolog**

### **Goal**

The goal of this setup is to create a local environment for translating natural language statements into Prolog code using OpenAI’s ChatGPT API and running the generated code with SWI-Prolog. Below is the process and the outcome of a baseline experiment.

---

### **Tool Setup**

1. **ChatGPT API**
    - **Access and API Key**: Created an OpenAI account and used the AIEA lab's API key for access.
    - **Documentation and Tutorials**: Reviewed [OpenAI's API documentation](https://platform.openai.com/docs/overview) and completed tutorials to understand how to work with the `v1/chat/completions` endpoint.
    - **Namespace Access**: Requested to join the organization’s OpenAI namespace (approval pending).
2. **SWI-Prolog Installation**
    - **Install and Path Setup**: Installed SWI-Prolog on macOS via Homebrew. Configured environment variables (`SWI_HOME_DIR`, `LD_LIBRARY_PATH`, and `DYLD_LIBRARY_PATH`) to ensure compatibility with Python.
    - **Troubleshooting**: Verified installation paths and resolved minor issues with environment settings.
3. **Python Integration with Prolog**
    - **pyswip Library**: Installed `pyswip` as a Python wrapper for SWI-Prolog, enabling Prolog queries within Python scripts.

---

### **Baseline Experiment Code**

```python
python
Copy code
import openai
from pyswip import Prolog
import os

os.environ["SWI_HOME_DIR"] = "/opt/homebrew/Cellar/swi-prolog/9.2.8/libexec"
os.environ["LD_LIBRARY_PATH"] = "/opt/homebrew/Cellar/swi-prolog/9.2.8/lib/swipl/lib/arm64-darwin"
os.environ["DYLD_LIBRARY_PATH"] = os.environ["LD_LIBRARY_PATH"]

openai.api_key = ''  

def generate_prolog(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "Translate the following statement into Prolog code."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100
    )
    return response.choices[0].message['content'].strip()

natural_language_query = "Translate this statement into Prolog: 'Everyone is mortal, and Socrates is a person.'"

prolog_code = generate_prolog(natural_language_query)
print("Generated Prolog Code:\n", prolog_code)

prolog = Prolog()

prolog.retractall("person(_)")
prolog.retractall("mortal(_)")

cleaned_prolog_code = prolog_code.replace('.', '').strip()

try:
    for clause in cleaned_prolog_code.splitlines():
        clause = clause.strip()
        if clause:
            prolog.assertz(clause)
    print("Prolog code asserted successfully.")
except Exception as e:
    print(f"Error asserting Prolog code: {e}")

try:
    result = list(prolog.query("mortal(socrates)."))
    print("Query Result:\n", result)
except Exception as e:
    print(f"Error querying Prolog: {e}")

```

---

### **Experiment Results**

- **Translation**: The prompt “Translate this statement into Prolog: 'Everyone is mortal, and Socrates is a person.'” generated Prolog code: `mortal(X) :- person(X). person(socrates).`
- **Execution**: The Prolog code was successfully asserted, and querying `mortal(socrates).` returned `true`, confirming that `socrates` is recognized as mortal.

### **Challenges & Solutions**

- **Syntax Adjustments**: Removed trailing periods in the Prolog code generated by OpenAI to avoid syntax errors during assertion.
- **Duplicate Assertions**: Used `retractall` to clear facts and rules between test runs, which prevented duplicate results in the output.
